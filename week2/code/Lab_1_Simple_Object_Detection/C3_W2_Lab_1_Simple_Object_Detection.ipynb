{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmANPR2jhCR6"
      },
      "source": [
        "# Simple Object Detection in Tensorflow\n",
        "\n",
        "This lab will walk you through how to use object detection models available in [Tensorflow Hub](https://www.tensorflow.org/hub). In the following sections, you will:\n",
        "\n",
        "* explore the Tensorflow Hub for object detection models\n",
        "* load the models in your workspace\n",
        "* preprocess an image for inference \n",
        "* run inference on the models and inspect the output\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DkMLuGDhCR6"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OEoRKdmByrb0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from PIL import Image\n",
        "from PIL import ImageOps\n",
        "import tempfile\n",
        "from six.moves.urllib.request import urlopen\n",
        "from six import BytesIO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb8MBgTOhCR6"
      },
      "source": [
        "### Download the model from Tensorflow Hub\n",
        "\n",
        "Tensorflow Hub is a repository of trained machine learning models which you can reuse in your own projects. \n",
        "- You can see the domains covered [here](https://tfhub.dev/) and its subcategories. \n",
        "- For this lab, you will want to look at the [image object detection subcategory](https://tfhub.dev/s?module-type=image-object-detection). \n",
        "- You can select a model to see more information about it and copy the URL so you can download it to your workspace. \n",
        "- We selected a [inception resnet version 2](https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1)\n",
        "- You can also modify this following cell to choose the other model that we selected, [ssd mobilenet version 2](https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C9pCzz4uy20U"
      },
      "outputs": [],
      "source": [
        "# you can switch the commented lines here to pick the other model\n",
        "\n",
        "# inception resnet version 2\n",
        "module_handle = \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\"\n",
        "\n",
        "# You can choose ssd mobilenet version 2 instead and compare the results\n",
        "#module_handle = \"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3trj5FbhCR6"
      },
      "source": [
        "#### Load the model\n",
        "\n",
        "Next, you'll load the model specified by the `module_handle`.\n",
        "- This will take a few minutes to load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0WHkGDHfhCR6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        }
      ],
      "source": [
        "model = hub.load(module_handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ey0FpHGhCR6"
      },
      "source": [
        "#### Choose the default signature\n",
        "\n",
        "Some models in the Tensorflow hub can be used for different tasks. So each model's documentation should show what *signature* to use when running the model. \n",
        "- If you want to see if a model has more than one signature then you can do something like `print(hub.load(module_handle).signatures.keys())`. In your case, the models you will be using only have the `default` signature so you don't have to worry about other types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X1BU7AGthCR6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KeysView(_SignatureMap({'default': <ConcreteFunction pruned(images) at 0x2A49EDF7A90>}))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# take a look at the available signatures for this particular model\n",
        "model.signatures.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfc9ax9hhCR6"
      },
      "source": [
        "Please choose the 'default' signature for your object detector.\n",
        "- For object detection models, its 'default' signature will accept a batch of image tensors and output a dictionary describing the objects detected, which is what you'll want here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pzwR5zE_hCR7"
      },
      "outputs": [],
      "source": [
        "detector = model.signatures['default']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvb-3r3thCR7"
      },
      "source": [
        "### download_and_resize_image\n",
        "\n",
        "This function downloads an image specified by a given \"url\", pre-processes it, and then saves it to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ucsxak_qhCR7"
      },
      "outputs": [],
      "source": [
        "def download_and_resize_image(url, new_width=256, new_height=256, context=None):\n",
        "    '''\n",
        "    Fetches an image online, resizes it and saves it locally.\n",
        "    \n",
        "    Args:\n",
        "        url (string) -- link to the image\n",
        "        new_width (int) -- size in pixels used for resizing the width of the image\n",
        "        new_height (int) -- size in pixels used for resizing the length of the image\n",
        "        \n",
        "    Returns:\n",
        "        (string) -- path to the saved image\n",
        "    '''\n",
        "    \n",
        "    \n",
        "    # create a temporary file ending with \".jpg\"\n",
        "    _, filename = tempfile.mkstemp(suffix=\".jpg\")\n",
        "    \n",
        "    # opens the given URL\n",
        "    response = urlopen(url, context=context)\n",
        "    \n",
        "    # reads the image fetched from the URL\n",
        "    image_data = response.read()\n",
        "    \n",
        "    # puts the image data in memory buffer\n",
        "    image_data = BytesIO(image_data)\n",
        "    \n",
        "    # opens the image\n",
        "    pil_image = Image.open(image_data)\n",
        "    \n",
        "    # resizes the image. will crop if aspect ratio is different.\n",
        "    pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n",
        "    \n",
        "    # converts to the RGB colorspace\n",
        "    pil_image_rgb = pil_image.convert(\"RGB\")\n",
        "    \n",
        "    # saves the image to the temporary file created earlier\n",
        "    pil_image_rgb.save(filename, format=\"JPEG\", quality=90)\n",
        "    \n",
        "    print(\"Image downloaded to %s.\" % filename)\n",
        "    \n",
        "    return filename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7qodEJHhCR7"
      },
      "source": [
        "### Download and preprocess an image\n",
        "\n",
        "Now, using `download_and_resize_image` you can get a sample image online and save it locally. \n",
        "- We've provided a URL for you, but feel free to choose another image to run through the object detector.\n",
        "- You can use the original width and height of the image but feel free to modify it and see what results you get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xHTDalVrhCR7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image downloaded to C:\\Users\\carlos\\AppData\\Local\\Temp\\tmpu9tllfp3.jpg.\n"
          ]
        }
      ],
      "source": [
        "import ssl\n",
        "\n",
        "# This restores the same behavior as before.\n",
        "context = ssl._create_unverified_context()\n",
        "\n",
        "# You can choose a different URL that points to an image of your choice\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/f/fb/20130807_dublin014.JPG\"\n",
        "\n",
        "# download the image and use the original height and width\n",
        "downloaded_image_path = download_and_resize_image(image_url, 3872, 2592, context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVNXUKMIhCR7"
      },
      "source": [
        "### run_detector\n",
        "\n",
        "This function will take in the object detection model `detector` and the path to a sample image, then use this model to detect objects and display its predicted class categories and detection boxes.\n",
        "- run_detector uses `load_image` to convert the image into a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wkkiQzKlhCR7"
      },
      "outputs": [],
      "source": [
        "def load_img(path):\n",
        "    '''\n",
        "    Loads a JPEG image and converts it to a tensor.\n",
        "    \n",
        "    Args:\n",
        "        path (string) -- path to a locally saved JPEG image\n",
        "    \n",
        "    Returns:\n",
        "        (tensor) -- an image tensor\n",
        "    '''\n",
        "    \n",
        "    # read the file\n",
        "    img = tf.io.read_file(path)\n",
        "    \n",
        "    # convert to a tensor\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    \n",
        "    return img\n",
        "\n",
        "\n",
        "def run_detector(detector, path):\n",
        "    '''\n",
        "    Runs inference on a local file using an object detection model.\n",
        "    \n",
        "    Args:\n",
        "        detector (model) -- an object detection model loaded from TF Hub\n",
        "        path (string) -- path to an image saved locally\n",
        "    '''\n",
        "    \n",
        "    # load an image tensor from a local file path\n",
        "    img = load_img(path)\n",
        "\n",
        "    # add a batch dimension in front of the tensor\n",
        "    converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
        "    \n",
        "    # run inference using the model\n",
        "    result = detector(converted_img)\n",
        "\n",
        "    # save the results in a dictionary\n",
        "    result = {key:value.numpy() for key,value in result.items()}\n",
        "\n",
        "    # print results\n",
        "    print(\"Found %d objects.\" % len(result[\"detection_scores\"]))\n",
        "\n",
        "    print(result[\"detection_scores\"])\n",
        "    print(result[\"detection_class_entities\"])\n",
        "    print(result[\"detection_boxes\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSEeJSkxhCR7"
      },
      "source": [
        "### Run inference on the image\n",
        "\n",
        "You can run your detector by calling the `run_detector` function. This will print the number of objects found followed by three lists: \n",
        "\n",
        "* The detection scores of each object found (i.e. how confident the model is), \n",
        "* The classes of each object found, \n",
        "* The bounding boxes of each object\n",
        "\n",
        "You will see how to overlay this information on the original image in the next sections and in this week's assignment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "csanHvDIz4_t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 100 objects.\n",
            "[0.65301365 0.6104641  0.6014318  0.5925076  0.5916552  0.58161587\n",
            " 0.5504253  0.4959837  0.47432423 0.47311807 0.44078818 0.40514985\n",
            " 0.39802992 0.3940052  0.37132725 0.361491   0.3613788  0.34719446\n",
            " 0.3338281  0.3123422  0.2887691  0.25730023 0.25728977 0.25199425\n",
            " 0.24779336 0.23425104 0.2042493  0.20337388 0.17981797 0.17967382\n",
            " 0.1737616  0.16424891 0.1603021  0.15897335 0.15624018 0.15473814\n",
            " 0.14758138 0.13620596 0.1274027  0.12552902 0.12113483 0.11808485\n",
            " 0.11387309 0.1122777  0.11129203 0.09723669 0.09130894 0.08968679\n",
            " 0.08878569 0.08632548 0.08333959 0.08098013 0.07982574 0.07747097\n",
            " 0.07737837 0.07630658 0.0750966  0.07389945 0.07228663 0.07189362\n",
            " 0.07107911 0.06929894 0.06824782 0.06428479 0.06251596 0.0622237\n",
            " 0.06204366 0.05941001 0.05796941 0.05780398 0.05723314 0.0534766\n",
            " 0.05302073 0.05247042 0.04890394 0.0481251  0.04571681 0.04425466\n",
            " 0.04337338 0.04277263 0.04263126 0.0417758  0.04081811 0.03973649\n",
            " 0.03946006 0.03944882 0.03862489 0.03766357 0.03759724 0.03569562\n",
            " 0.03359616 0.03329556 0.03275675 0.03229861 0.03136338 0.02979335\n",
            " 0.02860044 0.02855478 0.02825529 0.02786192]\n",
            "[b'Person' b'Person' b'Person' b'Person' b'Footwear' b'Person' b'Building'\n",
            " b'Bicycle' b'Window' b'Building' b'Person' b'Wheel' b'Building'\n",
            " b'Building' b'Person' b'Building' b'Wheel' b'Window' b'Window'\n",
            " b'Building' b'Person' b'Person' b'Van' b'Bicycle wheel' b'Person'\n",
            " b'Window' b'Window' b'Bicycle' b'Building' b'Window' b'Window' b'Man'\n",
            " b'Person' b'Person' b'Woman' b'Clothing' b'Bicycle wheel' b'Window'\n",
            " b'Person' b'Window' b'Land vehicle' b'Land vehicle' b'Clothing'\n",
            " b'Bicycle' b'Window' b'House' b'Land vehicle' b'Land vehicle' b'House'\n",
            " b'Man' b'Window' b'Clothing' b'Footwear' b'Person' b'Window' b'Man'\n",
            " b'Man' b'House' b'Person' b'Building' b'Clothing' b'Window' b'Person'\n",
            " b'Jeans' b'Man' b'Furniture' b'Person' b'Person' b'Person'\n",
            " b'Land vehicle' b'Person' b'Window' b'House' b'Woman' b'Window' b'Man'\n",
            " b'Person' b'Man' b'Clothing' b'Bicycle' b'Man' b'Person' b'Window'\n",
            " b'Person' b'Car' b'Man' b'Car' b'Chair' b'House' b'Window' b'Clothing'\n",
            " b'Tire' b'Clothing' b'Window' b'Land vehicle' b'Window' b'Man' b'Window'\n",
            " b'Bus' b'Clothing']\n",
            "[[5.1278621e-01 5.2925879e-01 6.0161972e-01 5.5207825e-01]\n",
            " [5.1962930e-01 6.0151273e-01 6.4617300e-01 6.3462579e-01]\n",
            " [5.0550830e-01 5.0044119e-01 6.0129303e-01 5.2308422e-01]\n",
            " [4.8632550e-01 4.1273028e-01 6.7883748e-01 4.5991924e-01]\n",
            " [8.1519073e-01 9.5612317e-01 8.4270328e-01 9.8714554e-01]\n",
            " [4.9540845e-01 9.2354828e-01 8.3568084e-01 9.9905014e-01]\n",
            " [1.1463700e-02 1.2221950e-02 7.3864645e-01 4.2463386e-01]\n",
            " [5.7766819e-01 3.6645702e-01 7.1276623e-01 4.8337358e-01]\n",
            " [0.0000000e+00 1.1926116e-01 2.2389516e-01 1.8392888e-01]\n",
            " [7.7412061e-02 4.1300562e-01 5.7952410e-01 5.6044394e-01]\n",
            " [5.1382232e-01 7.4803185e-01 5.9199166e-01 7.6661175e-01]\n",
            " [6.3213593e-01 3.5992590e-01 7.0386571e-01 4.1182798e-01]\n",
            " [0.0000000e+00 7.9704654e-01 6.7333907e-01 1.0000000e+00]\n",
            " [1.6018245e-02 6.8487734e-01 5.5876046e-01 8.1116635e-01]\n",
            " [5.0027770e-01 3.7696570e-01 6.3326675e-01 4.1450232e-01]\n",
            " [0.0000000e+00 2.1906698e-01 6.6040033e-01 4.3326530e-01]\n",
            " [6.4053774e-01 4.4508898e-01 7.0298129e-01 4.8344025e-01]\n",
            " [1.9325574e-03 0.0000000e+00 1.3937457e-01 2.6296820e-02]\n",
            " [2.5770888e-03 9.6666843e-01 1.5373006e-01 9.9999923e-01]\n",
            " [5.5857340e-04 1.5130980e-03 7.6519978e-01 2.7001128e-01]\n",
            " [5.0452685e-01 3.6118796e-01 6.3473386e-01 3.8534227e-01]\n",
            " [4.9806973e-01 3.6458170e-01 6.6123229e-01 4.0498099e-01]\n",
            " [4.8340923e-01 6.1965275e-01 5.6269950e-01 6.6155708e-01]\n",
            " [6.3127929e-01 3.6036599e-01 7.0415372e-01 4.1150030e-01]\n",
            " [5.2181566e-01 5.7764864e-01 5.8759886e-01 6.0071832e-01]\n",
            " [2.1956792e-01 3.4874424e-01 3.3837342e-01 3.7707490e-01]\n",
            " [1.2486064e-01 2.5091076e-01 2.7993509e-01 2.8158179e-01]\n",
            " [5.7718760e-01 3.6229506e-01 7.0702070e-01 4.4182444e-01]\n",
            " [2.5747520e-01 5.6757003e-01 5.3110480e-01 6.8772429e-01]\n",
            " [4.2060517e-02 8.7477142e-01 2.5276774e-01 9.1303039e-01]\n",
            " [1.5635018e-01 4.4340014e-01 2.2221035e-01 4.7578397e-01]\n",
            " [5.0196272e-01 9.2148721e-01 8.3640444e-01 1.0000000e+00]\n",
            " [5.2362317e-01 5.7025909e-01 5.8451730e-01 5.9158278e-01]\n",
            " [5.1324594e-01 6.7927444e-01 5.5099446e-01 6.9258147e-01]\n",
            " [5.1912230e-01 5.9998792e-01 6.4637417e-01 6.3403451e-01]\n",
            " [5.2430797e-01 9.2496359e-01 8.1077284e-01 9.9799788e-01]\n",
            " [6.3818586e-01 4.4292006e-01 7.0165336e-01 4.8409814e-01]\n",
            " [3.4215208e-02 3.5557383e-01 1.6225627e-01 3.7492096e-01]\n",
            " [4.8848256e-01 4.5349663e-01 6.2179130e-01 4.7972611e-01]\n",
            " [9.3071943e-04 3.0770007e-01 1.0653242e-01 3.3205855e-01]\n",
            " [4.8301077e-01 6.1990917e-01 5.6477630e-01 6.6069645e-01]\n",
            " [5.8219337e-01 3.6493331e-01 7.1387744e-01 4.8470649e-01]\n",
            " [5.2354908e-01 7.4919707e-01 5.8537936e-01 7.6531726e-01]\n",
            " [6.0915965e-01 4.2669633e-01 7.0516908e-01 4.8708925e-01]\n",
            " [3.5137066e-01 9.7485673e-01 5.5312836e-01 9.9887735e-01]\n",
            " [0.0000000e+00 8.1122512e-01 6.8640566e-01 9.9714935e-01]\n",
            " [5.7629979e-01 3.5746357e-01 7.0481765e-01 4.4029251e-01]\n",
            " [5.6488872e-01 3.6302340e-01 7.0865315e-01 4.1603523e-01]\n",
            " [1.0895869e-02 2.3306422e-02 7.2650486e-01 4.2174068e-01]\n",
            " [4.8468086e-01 4.1068703e-01 6.9468236e-01 4.6309155e-01]\n",
            " [8.0975093e-02 3.8471192e-01 2.0780905e-01 4.1174826e-01]\n",
            " [5.3828329e-01 6.0357362e-01 6.3477439e-01 6.3440686e-01]\n",
            " [6.2984145e-01 6.1496735e-01 6.4493424e-01 6.2538338e-01]\n",
            " [5.0275731e-01 3.8239419e-01 5.9614694e-01 4.1272342e-01]\n",
            " [0.0000000e+00 1.2449059e-02 1.4020798e-01 2.4740001e-02]\n",
            " [5.1444441e-01 7.4779242e-01 5.9198582e-01 7.6682776e-01]\n",
            " [5.0618005e-01 5.0040680e-01 6.0068643e-01 5.2331221e-01]\n",
            " [0.0000000e+00 2.1129170e-01 6.5079093e-01 4.3428439e-01]\n",
            " [4.8945579e-01 4.5439214e-01 5.7233971e-01 4.7647181e-01]\n",
            " [0.0000000e+00 7.0623618e-01 6.1702192e-01 8.6623985e-01]\n",
            " [5.0917047e-01 4.1627958e-01 6.6930944e-01 4.5959616e-01]\n",
            " [4.6524429e-03 8.0309242e-01 1.5984096e-01 8.4040016e-01]\n",
            " [5.2614850e-01 5.6835186e-01 5.7943970e-01 5.8281076e-01]\n",
            " [6.7192000e-01 9.4027984e-01 8.2127374e-01 9.8925024e-01]\n",
            " [5.0276625e-01 3.7388343e-01 6.4698321e-01 4.1297230e-01]\n",
            " [5.7423514e-01 2.6739842e-01 6.5776712e-01 3.2031542e-01]\n",
            " [4.8604631e-01 4.4450790e-01 6.2478024e-01 4.7350240e-01]\n",
            " [5.1724666e-01 7.5695807e-01 5.8852077e-01 7.7146357e-01]\n",
            " [5.2337706e-01 5.5784845e-01 5.7914096e-01 5.7353961e-01]\n",
            " [6.1246043e-01 4.2732659e-01 7.0608491e-01 4.8825011e-01]\n",
            " [5.2412075e-01 5.6154478e-01 5.7838631e-01 5.8046836e-01]\n",
            " [0.0000000e+00 2.4423572e-01 6.0777348e-02 2.9361373e-01]\n",
            " [1.4871826e-02 2.1489945e-03 7.4543214e-01 2.5981086e-01]\n",
            " [4.9324903e-01 9.2395115e-01 8.3710033e-01 9.9775559e-01]\n",
            " [8.3741937e-03 2.4216874e-01 4.9729757e-02 2.8316066e-01]\n",
            " [5.0532955e-01 3.6017555e-01 6.4357054e-01 3.9146343e-01]\n",
            " [5.1310104e-01 5.2379411e-01 6.0050738e-01 5.4296803e-01]\n",
            " [5.2041996e-01 6.0097992e-01 6.4612418e-01 6.3436681e-01]\n",
            " [5.1822174e-01 5.0339174e-01 5.9754944e-01 5.2268261e-01]\n",
            " [5.9419358e-01 3.6132777e-01 7.0546442e-01 4.1585526e-01]\n",
            " [5.1325583e-01 6.7931539e-01 5.5053377e-01 6.9248307e-01]\n",
            " [5.2229261e-01 5.3619021e-01 5.9757030e-01 5.5316138e-01]\n",
            " [4.2986980e-01 8.2870108e-01 5.8993256e-01 8.6432219e-01]\n",
            " [5.0489438e-01 3.8941965e-01 6.1507738e-01 4.1993585e-01]\n",
            " [5.2658916e-01 6.2717795e-01 5.6329793e-01 6.5372771e-01]\n",
            " [5.0130427e-01 3.6419398e-01 6.5995908e-01 4.0380156e-01]\n",
            " [5.1517016e-01 6.2410593e-01 5.6379259e-01 6.5800202e-01]\n",
            " [5.7313299e-01 2.6690072e-01 6.6616362e-01 3.1863800e-01]\n",
            " [8.3406448e-02 4.0741596e-01 5.8408296e-01 5.5851167e-01]\n",
            " [2.8820065e-01 4.7747366e-04 4.1436911e-01 3.6599807e-02]\n",
            " [4.9727491e-01 4.5529726e-01 5.8382541e-01 4.7793654e-01]\n",
            " [6.2716842e-01 3.6102384e-01 7.0599234e-01 4.0978059e-01]\n",
            " [5.1586318e-01 3.8005626e-01 5.9689468e-01 4.1175750e-01]\n",
            " [1.1812525e-02 3.0812320e-01 9.7284585e-02 3.2503858e-01]\n",
            " [5.1249903e-01 6.2365198e-01 5.6242204e-01 6.5764230e-01]\n",
            " [4.0100968e-01 8.8508874e-01 5.8127725e-01 9.3921685e-01]\n",
            " [5.1385123e-01 5.2948469e-01 6.0200673e-01 5.5236310e-01]\n",
            " [0.0000000e+00 1.0061469e-02 1.3615663e-01 3.1600825e-02]\n",
            " [4.8043132e-01 6.2042457e-01 5.6528485e-01 6.6014796e-01]\n",
            " [5.1935714e-01 3.6184052e-01 6.2499744e-01 3.8491902e-01]]\n"
          ]
        }
      ],
      "source": [
        "# runs the object detection model and prints information about the objects found\n",
        "run_detector(detector, downloaded_image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "C3_W2_Lab_1_Simple_Object_Detection.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
